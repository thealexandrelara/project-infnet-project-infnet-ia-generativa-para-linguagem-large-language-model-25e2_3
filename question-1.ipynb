{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 - Fundamentos das LLMs\n",
    "\n",
    "Questão 1: Explique os seguintes conceitos fundamentais dos LLMs, fornecendo exemplos práticos e diagramas onde for relevante:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pre-training é a etapa inicial de treinamento de LLMs. O modelo é treinado com uma quantidade gigante de dados não rotulados, que podem vir da internet, de livros, de artigos, de notícias, ou mesmo fontes físicas que forem convertidas para o meio digital. Nessa etapa, a ideia não é fazer com que o modelo se especialize em uma tarefa em específico, mas sim que ele seja capaz de compreender padrões da linguagem, como gramática, estrutura das frases, as relações semânticas, fatos do mundo e até mesmo vieses que estiverem presentes nos dados. \n",
    "\n",
    "Para entender um pouco mais de como isso funciona, vamos supor que eu forneço pro modelo a frase \"O mercado financeiro é muito\", nesse caso, o modelo tenta prever a próxima palavra com base no que ele viu durante o pre-training, nesse caso ele poderia preever palavras como \"volátil\", \"lucrativo\", \"difícil\", etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No transfer learning ocorre um aproveitamento do conhecimento adquirido em uma tarefa (como no pre-training para se obter o entendimento da linguagem geral) para poder aplicar esse conhecimento em uma tarefa que seja mais específica evitando que seja necessário treinar um modelo do zero para cada aplicação nova. \n",
    "\n",
    "Por exemplo, supondo que um modelo foi pré-treinado em um vasto corpus de texto em português você pode adaptar este modelo pra poder fazer análise de sentimentos de reviews de produtos em sites brasileiros. Basicamente, ao invés de ter que fazer o modelo aprender sobre a gramática e vocubulário do zero, ele vai focar em aprender a como associar as palavras e frases em sentimentos positivos, negativos ou neutros, utilizando um conjunto de dados bem menor de reviews que já foram classificados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings são representações numéricas de palavras, frases ou até mesmo sentenças inteiras em um espaço vetorial multidimensional. Tem como característica capturar relações semânticas entre as palavras, o que faz com que palavras com significados parecidos ou que são usadas em contextos parecidos terem representações vetoriais próximas nesse espaço.\n",
    "\n",
    "Como exemplo, as palavras \"rei\" e \"rainha\" estariam próximas no espaço vetorial, e inclusive seriam semelhantes à relação entre \"homem\" e \"mulher\". Já se pegássemos palavras como \"carro\" e \"árvore\" estas estariam mais distantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers trata-se de uma arquitetura de rede neural que revolucionou o processamento de linguagem natural. Ela foi introduziada pelo artigo \"Attention is All You Need\" em 2017. Basicamente, Transformers processam sequênticas inteiras de texto simultaneamente usando mecanismos de Attention (que será explicado mais abaixo), o que permite a captura de dependências de longo alcance com maior eficiência.\n",
    "\n",
    "Por exemplo, considerando a frase \"O gato deitou na cama\", uma arquitetura Transformer consegue processar todas essas palavras simultaneamente, e consegue compreender que \"gato\" é o sujeito que executa a ação, enquanto que a \"cama\" é onde a ação ocorre. Claro que nesse exemplo, temos uma frase curta, mas em um contexto de texto mais longo onde as palavras estão mais distantes, modelos anteriores poderiam \"esquecer\" as informações iniciais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pode-se dizer que o mecanismo de Attention é o coração da arquitetura Transformer. Ele permite que o modelo foque em partes relevantes da entrada ao processar cada elemento, calculando pesos de importância e permitindo conexões diretas entre elementos distantes. Em essência, ele \"presta atenção\" às palavras mais relevantes para a tarefa em questão, independentemente de sua posição na frase. \n",
    "\n",
    "Como exemplo, podemos citar a tradução da frase \"The cat sat on the mat\" para o português \"O gato sentou no tapete\". O mecanismo de Attention aprende que ao gerar gato, ele tem que focar principalmente em \"cat\" e ao gerar \"tapete\" ele tem que focar em \"mat\". Outro exemplo, é na frase \"O gato não atravessou a rua porque estava muito cansado\", para tentar entender quem estava \"cansado\" o mecanismo de Attention iri atribuir um peso maior à palavra \"gato\" ao invés de \"rua\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este é o processo de ajustar um modelo pré-treinado para uma tarefa específica usando um conjunto de dados menor e específico para realizar bem esta tarefa específica. Trata-se de uma especialização do Transfer Learning onde os pesos do modelo são refinados para a aplicação desejada.\n",
    "\n",
    "Exemplo: Pegar um modelo de LLM pré-treinado e \"fine-tunar\" ele com conversas de atendimento ao cliente de uma empresa para criar um chatbot especializado. A ideia é pegar um dataset menor com milhares de exemplos contendo interações entre clientes e o suporte da empresa, e ajustar os pesos para gerar respostas mais apropriadas para esse contexto especificamente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
